{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9be39ab3-61c0-4ebd-8397-d31db7133b78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating Core schema to persist semiraw data\n",
    "spark.sql(\"DROP TABLE IF EXISTS ifood_db_ws.core.nyc_yellow_taxi\")\n",
    "\n",
    "spark.sql(\"\"\"CREATE TABLE IF NOT EXISTS ifood_db_ws.core.nyc_yellow_taxi (\n",
    "  vendor_id STRING,\n",
    "  tpep_pickup_datetime TIMESTAMP,\n",
    "  tpep_dropoff_datetime TIMESTAMP,\n",
    "  total_amount FLOAT,\n",
    "  passenger_count FLOAT\n",
    ")\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0508eda-28d4-428f-a60e-82c96fb88fc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "from pyspark.sql.functions import col, lit, when\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Loading config file \n",
    "with open(\"/Workspace/Users/arturvieirasousa@gmail.com/ifd-tech-ch/src/config.json\", \"r\") as f:\n",
    "    conf_file = json.load(f)\n",
    "\n",
    "# Defining parameters\n",
    "app_token = conf_file[\"app_token\"]\n",
    "start_date = conf_file[\"start_date\"]\n",
    "end_date = conf_file[\"end_date\"]\n",
    "api_url = conf_file[\"api_url\"]\n",
    "\n",
    "# Defining limits to the api calls\n",
    "batch_size = 50000\n",
    "wait_time = 5\n",
    "\n",
    "# Defining SOQL Query\n",
    "order_by = \"tpep_pickup_datetime\"\n",
    "select_clause = \"vendorid, passenger_count, total_amount, tpep_pickup_datetime, tpep_dropoff_datetime\"\n",
    "where_clause = f\"tpep_pickup_datetime >= '{start_date}' AND tpep_dropoff_datetime <= '{end_date}'\"\n",
    "\n",
    "# Fetch Data\n",
    "offset = 0\n",
    "total_rows_fetched = 0\n",
    "\n",
    "while True:\n",
    "    api_params = {\n",
    "        \"$$app_token\": app_token,\n",
    "        \"$select\": select_clause,\n",
    "        \"$where\": where_clause,\n",
    "        \"$limit\": batch_size,\n",
    "        \"$offset\": offset,\n",
    "        \"$order\": f\"{order_by} ASC\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(api_url, params=api_params)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        data_rows = response.json()\n",
    "        \n",
    "        # Creating a PySpark DataFrame\n",
    "        df = spark.createDataFrame(data_rows)\n",
    "\n",
    "        # Converting data types lost during the API call\n",
    "        ready_to_append = df.withColumnRenamed(\"VendorID\", \"vendor_id\") \\\n",
    "        .withColumn(\"vendor_id\", col(\"vendor_id\").cast(\"string\")) \\\n",
    "        .withColumn(\"tpep_pickup_datetime\", col(\"tpep_pickup_datetime\").cast(\"timestamp\")) \\\n",
    "        .withColumn(\"tpep_dropoff_datetime\", col(\"tpep_dropoff_datetime\").cast(\"timestamp\")) \\\n",
    "        .withColumn(\"total_amount\", col(\"total_amount\").cast(\"float\")) \\\n",
    "        .withColumn(\"passenger_count\", col(\"passenger_count\").cast(\"float\"))\n",
    "\n",
    "        ready_to_append.write.mode(\"append\").saveAsTable(\"ifood_db_ws.core.nyc_yellow_taxi\")\n",
    "        \n",
    "        rows_in_batch = len(data_rows)\n",
    "\n",
    "        if rows_in_batch == 0:\n",
    "            print(\"\\nExtraction complete: the API returned no rows.\")\n",
    "            break\n",
    "\n",
    "        # Adding the current batch to the main list\n",
    "\n",
    "        total_rows_fetched += rows_in_batch\n",
    "        \n",
    "        print(f\"Batch {offset // batch_size + 1} with {rows_in_batch} rows fetched. Total rows fetched: {total_rows_fetched}\")\n",
    "\n",
    "        # After the last batch was added to the list containing all data, \n",
    "        # if the batch size is lower than the limit defined on the config file, the process will stop\n",
    "        if rows_in_batch < batch_size:\n",
    "            print(\"Extraction complete.\")\n",
    "            break\n",
    "\n",
    "        # Define the new offset, for the next iteration\n",
    "        offset += batch_size\n",
    "        \n",
    "        print(f\"Pausing for {wait_time} seconds...\")\n",
    "        time.sleep(wait_time)\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"\\nError fetching data at offset {offset}: {e}\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unexpected error occurred: {e}\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nData Processing\")\n",
    "print(f\"Total {total_rows_fetched} rows were fetched through the API.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "nyc_taxi_ingestion.py",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
